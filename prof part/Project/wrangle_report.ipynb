{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "###  [About](#About)\n",
    "### [Project Goal](#Goal)\n",
    "#### [Step:1 Gather the data](#Gather)\n",
    "#### [Step:2 Asses the Data](#Asses)\n",
    "#### [Step:3 Data Cleaning](#Cleaning)\n",
    "### [Analyzing, and Visualizing Data](#Analyzing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='About'></a>\n",
    "## About\n",
    "this project is part of the Data Analysis Professional Nanodegree Program that is supported by ITIDA  under the FWD (tech learning and freelancing upskilling scholarship)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Goal'></a>\n",
    "## Project Goal\n",
    "\n",
    "wrangle **WeRateDogs Twitter data** to create interesting and trustworthy analyses and visualizations. <br> The Twitter archive is great, but it only contains very basic tweet information.\n",
    "<br>\n",
    "Additional gathering, then assessing and cleaning is required for \"_Wow!_\"-worthy analyses and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Gather'></a>\n",
    "## Step:1 Gather the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Enhanced Twitter Archive** <br>\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. <br> One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" Of the 5000+ tweets, I have filtered for tweets with ratings only (there are 2356).\n",
    "2. **Additional Data via the Twitter API** <br> \n",
    "retweet count and favorite count are two of the notable column omissions.\n",
    "3. **Image Predictions File** <br>\n",
    "they ran every image in the WeRateDogs Twitter archive through a [neural network](https://www.youtube.com/watch?v=2-Ol7ZB0MmU) that can classify breeds of dogs*. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.  The WeRateDogs Twitter archive. I am giving this file to you, so imagine it as a file on hand. Download this file manually by clicking the following link: [`twitter_archive_enhanced.csv`](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Each tweet's entire set of JSON data (with at minimum tweet ID, retweet count, and \n",
    "favorite count) in a file called 'tweet_json.txt' were stored using Twitter API and \n",
    "Python's Tweepy library. Each tweet's JSON data was written to its own line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.  The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (`image_predictions.tsv`) is hosted on Udacity's servers and should be downloaded programmatically using the [Requests](https://pypi.org/project/requests/) library and the following URL: [https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Asses'></a>\n",
    "##  Step:2 Asses the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually (visually)\n",
    "using the following methods on the 3 data frames \n",
    "1. .sample(10)\n",
    "2. .describe()\n",
    "3. .info()\n",
    "\n",
    "\n",
    "\n",
    "### programaticly \n",
    "using the following methods \n",
    "1. .isnull()\n",
    "2. .value_counts()\n",
    "\n",
    "### result of assess\n",
    "found the following problems ann issues\n",
    "\n",
    "#### Data Tidiness Issues\n",
    "\n",
    "- The 3 sources of the data should be in one dataframe. so i need to merge them together \n",
    "- Dog types should be in one column \n",
    "- Separate timestamp into day - month - year (3 columns)\n",
    "- create full dog rating\n",
    "\n",
    "#### Data Quality Issues\n",
    "\n",
    "- alot of missing values in some column\n",
    "- change datestamp into datetime \n",
    "- Change tweet_id from an integer to a string\n",
    "- missing data for the dog breed predictions\n",
    "- Many dog breed names are in lowercase in the p1, the p2 and the p3 columns. \n",
    "- Capitalize all words found in columns 'p1', 'p2' and 'p3'.\n",
    "- Some names in the name columns can't possibly be dog names. (a, the, an, this...)\n",
    "- we dont need the retweet only original tweets counts \n",
    "- there is some tweets with no images \n",
    "- scrapping the text errors (manual fix) some dog rating is wrong scrapped form original tweets\n",
    "- see and drop uneeded columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Cleaning'></a>\n",
    "## Step:3 Data Cleaning\n",
    "\n",
    "- Merge the json_data dataframe, the twitter dataframe, and the pred dataframe in new DF to save an old copy\n",
    "- clean the 4 types of dogs column into one column throw <br> \n",
    "gonna do that by extract the string from the 4 columns and add them into single column wiche will take none if all none or value if the record has value\n",
    "- change datestamp into datetim using `pd.DatetimeIndex(df['timestamp'])` \n",
    "- covert time stamp column into 3 separate columns with year , month and day. by creating new column with methods `dt.year , dt.month , dt.day`\n",
    "- convert the rating type into float using `.astype(float)`\n",
    "- clean the columns with alot of missing values by <br> droping any column with more than 70 % missing values \n",
    "- Change tweet_id from an integer to a string using `.astype(str)`\n",
    "- missing data for the dog breed predictions by <br> Droping rows that do not have a jpg_url column as tweets without a jpg_url do not contain images\n",
    "- Capitalize all words found in columns 'p1', 'p2' and 'p3'. `.str.title()`\n",
    "- some of the tweets were scrapped with wrong rating cleaning them by edit them manually\n",
    "- ctreate full dog rating by <br> rating_numerator /rating_denominator\n",
    "- some dog rating_numerator is very high manually find and edit <br> deleted the wrong one <br> edited what needed to be edit manually\n",
    "- drop name for each dog named 'a' as it was wrong names "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
